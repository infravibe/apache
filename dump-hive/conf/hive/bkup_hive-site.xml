<?xml version="1.0"?>
<configuration>
    <!-- Metastore Configuration -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://postgres:5432/metastore</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
        <description>Username to use against metastore database</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive</value>
        <description>Password to use against metastore database</description>
    </property>

    <property>
        <name>hive.metastore.db.type</name>
        <value>POSTGRES</value>
        <description>
            Type of database for metastore. Natively supported types are DERBY, MYSQL, MSSQL, ORACLE, POSTGRES.
            This is used by schematool to determine which DDL scripts to run.
        </description>
    </property>

    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hive-metastore:9083</value>
        <description>Thrift URI for the remote metastore</description>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>hdfs://namenode:9820/user/hive/warehouse</value>
        <description>Location of default database for the warehouse</description>
    </property>

    <!-- HDFS config -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:9820</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
        <description>Enforce metastore schema version consistency</description>
    </property>

    <property>
        <name>hive.metastore.schema.verification.record.version</name>
        <value>false</value>
        <description>Whether to record the schema version</description>
    </property>

    <!-- HiveServer2 Configuration -->
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
        <description>Port number of HiveServer2 Thrift interface</description>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>0.0.0.0</value>
        <description>Bind host on which to run the HiveServer2 Thrift service</description>
    </property>

    <property>
        <name>hive.server2.webui.enable</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.server2.webui.host</name>
        <value>0.0.0.0</value>
        <description>Host interface on which HiveServer2 web interface will listen</description>
    </property>

    <property>
        <name>hive.server2.webui.port</name>
        <value>10002</value>
        <description>Port number of HiveServer2 web interface</description>
    </property>

    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
        <description>Impersonate the connected user</description>
    </property>

    <property>
        <name>hive.server2.authentication</name>
        <value>NONE</value>
        <description>Client authentication types</description>
    </property>

    <property>
        <name>hive.server2.transport.mode</name>
        <value>binary</value>
        <description>Transport mode of HiveServer2</description>
    </property>

    <!-- Execution Configuration -->
    <property>
        <name>hive.execution.engine</name>
        <value>mr</value>
        <description>Execution engine to use (mr/tez/spark)</description>
    </property>

    <property>
        <name>hive.exec.scratchdir</name>
        <value>hdfs://namenode:9820/user/hive/tmp</value>
        <description>HDFS scratch space for Hive jobs</description>
    </property>

    <property>
        <name>hive.querylog.location</name>
        <value>hdfs://namenode:9820/user/hive/log</value>
        <description>Directory where query logs are stored</description>
    </property>

    <!-- Security Configuration -->
    <property>
        <name>hive.security.authorization.enabled</name>
        <value>false</value>
        <description>Enable or disable the Hive client authorization</description>
    </property>

    <property>
        <name>hive.users.in.admin.role</name>
        <value>hive</value>
        <description>List of users who have admin role</description>
    </property>

    <!-- Performance Configuration -->
    <property>
        <name>hive.vectorized.execution.enabled</name>
        <value>true</value>
        <description>Enable vectorized query execution</description>
    </property>

    <property>
        <name>hive.vectorized.execution.reduce.enabled</name>
        <value>true</value>
        <description>Enable vectorized reduce execution</description>
    </property>

    <property>
        <name>hive.cbo.enable</name>
        <value>true</value>
        <description>Enable cost-based optimizer for Hive</description>
    </property>

    <property>
        <name>hive.compute.query.using.stats</name>
        <value>true</value>
        <description>Use table statistics to answer queries</description>
    </property>

    <property>
        <name>hive.stats.autogather</name>
        <value>true</value>
        <description>Automatically gather table statistics</description>
    </property>

    <!-- Dynamic Partitioning -->
    <property>
        <name>hive.exec.dynamic.partition</name>
        <value>true</value>
        <description>Enable dynamic partitioning</description>
    </property>

    <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
        <description>Dynamic partitioning mode</description>
    </property>

    <property>
        <name>hive.exec.max.dynamic.partitions</name>
        <value>1000</value>
        <description>Maximum number of dynamic partitions</description>
    </property>

    <property>
        <name>hive.exec.max.dynamic.partitions.pernode</name>
        <value>100</value>
        <description>Maximum number of dynamic partitions per node</description>
    </property>
</configuration>
